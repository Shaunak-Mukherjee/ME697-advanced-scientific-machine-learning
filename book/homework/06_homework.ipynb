{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "hide-input",
          "hide-output"
        ],
        "id": "lnLRWWTdIJ20"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import matplotlib_inline\n",
        "matplotlib_inline.backend_inline.set_matplotlib_formats('png')\n",
        "import seaborn as sns\n",
        "sns.set_context(\"paper\")\n",
        "sns.set_style(\"ticks\");\n",
        "!pip install jaxtyping\n",
        "!pip install numpyro\n",
        "!pip install blackjax\n",
        "import optax\n",
        "from jax import value_and_grad\n",
        "from tqdm import trange\n",
        "from typing import Callable\n",
        "import blackjax\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax import random\n",
        "import pandas as pd\n",
        "import arviz as az\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from jax import vmap\n",
        "from jax import random as jrandom\n",
        "from jax._src.prng import PRNGKeyArray\n",
        "from jax.numpy import ndarray as Array\n",
        "from jaxtyping import Array, Float, PyTree, PRNGKeyArray\n",
        "from jax.flatten_util import ravel_pytree"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zvpXN8tpIJ22"
      },
      "source": [
        "# Homework 6\n",
        "\n",
        "## References\n",
        "\n",
        "+ Module 5: Inverse problems in deterministic scientifc models\n",
        "   - Inverse problems basics\n",
        "   - Sampling from posteriors\n",
        "   - Variational inference\n",
        "   - Deterministic, finite-dimensional dynamical systems\n",
        "   <!-- - PDE-constrained inverse problems -->\n",
        "   <!-- - Purely data-driven learning of dynamical systems -->\n",
        "\n",
        "## Instructions\n",
        "\n",
        "+ Type your name and email in the \"Student details\" section below.\n",
        "+ Develop the code and generate the figures you need to solve the problems using this notebook.\n",
        "+ For the answers that require a mathematical proof or derivation you should type them using latex. If you have never written latex before and you find it exceedingly difficult, we will likely accept handwritten solutions.\n",
        "+ The total homework points are 100. Please note that the problems are not weighed equally.\n",
        "\n",
        "## Student details\n",
        "\n",
        "+ **First Name: Shaunak**\n",
        "+ **Last Name: Mukherjee**\n",
        "+ **Email: mukher86@purdue.edu**\n",
        "+ **Used generative AI to complete this assignment (Yes/No): Yes**\n",
        "+ **Which generative AI tool did you use (if applicable)?: chatGPT**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-bfjuAF1IJ24"
      },
      "source": [
        "# Problem 1 - Why does the Metropolis algorithm work\n",
        "\n",
        "The objective of this problem is to understand why the Metropolis algorithm works.\n",
        "\n",
        "Consider a Markov chain $x_n$ with transition probabilities $p(x_{n+1}|x_n)$ and a probability density $\\pi(x)$.\n",
        "We say that $x_n$ has stationary distribution $\\pi$ if:\n",
        "\n",
        "$$\n",
        "\\pi(x_{n+1}) = \\int p(x_{n+1}|x_n)\\pi(x_n)dx_n.\n",
        "$$\n",
        "\n",
        "Intuitively, we can think of the equation above as follows.\n",
        "If we, somehow, sample $x_n$ from $\\pi$ and then sample $x_{n+1}$ from the transition probability $p(x_{n+1}|x_n)$, then $x_{n+1}$ is also a sample from $\\pi(x)$.\n",
        "It is like once we have a sample $\\pi$ sampling the Markov chain keeps giving us samples from $\\pi$.\n",
        "\n",
        "We say that the Markov chain $x_n$ is *reversible* with respect to $\\pi$ (equivalently, satisfies the *detailed balance* condition) with respect to $\\pi$, if:\n",
        "\n",
        "$$\n",
        "p(x_{n+1}|x_n)\\pi(x_n) = p(x_n|x_{n+1})\\pi(x_{n+1}).\n",
        "$$\n",
        "\n",
        "Intuitively, this condition means that going from sampling $x_{n}$ from $\\pi$ and transition to $x_{n+1}$ has the same probability as doing the inverse."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cR3LJ1ZvIJ25"
      },
      "source": [
        "## Part A - Prove that detailed balance implies stationarity\n",
        "\n",
        "Suppose that the Markov chain $x_n$ satisfies the detailed balance condition with respect to $\\pi$. Prove that $\\pi$ is a stationary distribution of the Markov chain.\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "Whatsapp bro\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bcrmzt7WIJ25"
      },
      "source": [
        "*Your answer here*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8pOPGvvCIJ26"
      },
      "source": [
        "## Part B - The Metropolis-Hastings transition kernel\n",
        "\n",
        "Let $\\pi(x)$ be the target distribution.\n",
        "Let $q(\\tilde{x}_{n+1}|x_n)$ be a proposal distribution of the Metropolis-Hastings algorithm.\n",
        "\n",
        "The Metropolis-Hastings algorithm results in a Markov chain $x_n$ defined as follows:\n",
        "\n",
        "+ Sample $\\tilde{x}_{n+1} \\sim q(\\tilde{x}_{n+1}|x_n)$\n",
        "+ Accept $\\tilde{x}_{n+1}$ and set $x_{n+1} = \\tilde{x}_{n+1}$ with probability $\\alpha(x_n, \\tilde{x}_{n+1})$\n",
        "+ Reject $\\tilde{x}_{n+1}$ and set $x_{n+1} = x_n$ with probability $1-\\alpha(x_n, \\tilde{x}_{n+1}),$\n",
        "\n",
        "\n",
        "where\n",
        "\n",
        "$$\n",
        "\\alpha(x_n, \\tilde{x}_{n+1}) = \\min\\left(1, \\frac{\\pi(\\tilde{x}_{n+1})q(x_n|\\tilde{x}_{n+1})}{\\pi(x_n)q(\\tilde{x}_{n+1}|x_n)}\\right).\n",
        "$$\n",
        "\n",
        "The purpose of this problem is to show that the transition kernel of the resulting Markov chain satisfies the detailed balance condition with respect to $\\pi$, and thus $\\pi$ is its stationary distribution.\n",
        "\n",
        "### B.I - Derive the transition kernel of the Metropolis algorithm\n",
        "\n",
        "Show that the transition kernel of the Metropolis algorithm is:\n",
        "\n",
        "$$\n",
        "p(x_{n+1}|x_n) = \\alpha(x_n, x_{n+1})q(x_{n+1}|x_n) +\n",
        "\\delta(x_{n+1} - x_n)\\int (1 - \\alpha(x_n, \\tilde{x}_{n+1}))q(\\tilde{x}_{n+1}|x_n)d\\tilde{x}_{n+1},\n",
        "$$\n",
        "\n",
        "where $\\delta$ is the Dirac delta function.\n",
        "\n",
        "Hints:\n",
        "\n",
        "+ Introduce an intermediate variable $i$ that takes the value $1$ if the proposed move is accepted and $0$ otherwise. That is:\n",
        "\n",
        "$$\n",
        "i | x_n, \\tilde{x}_{n+1} \\sim \\begin{cases}\n",
        "    1 & \\text{with probability } \\alpha(x_n, \\tilde{x}_{n+1}) \\\\\n",
        "    0 & \\text{with probability } 1 - \\alpha(x_n, \\tilde{x}_{n+1}).\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "+ Write the joint distribution $p(x_{n+1}| i, x_n, \\tilde{x}_{n+1})$ in terms of $i$ and $\\tilde{x}_{n+1}$:\n",
        "\n",
        "$$\n",
        "p(x_{n+1}| i, x_n, \\tilde{x}_{n+1}) = [\\delta(x_{n+1} - \\tilde{x}_{n+1})]^i [\\delta(x_{n+1} - x_n)]^{1-i}.\n",
        "$$\n",
        "\n",
        "+ Use the sum rule to express $p(x_{n+1}|x_n)$ in terms of $i$ and $\\tilde{x}_{n+1}$:\n",
        "\n",
        "$$\n",
        "p(x_{n+1}|x_n) = \\int \\sum_i p(x_{n+1}| i, x_n, \\tilde{x}_{n+1}) p(i | x_n, \\tilde{x}_{n+1}) q(\\tilde{x}_{n+1}|x_n) d\\tilde{x}_{n+1}.\n",
        "$$\n",
        "\n",
        "+ Use the definition of the Dirac delta function to simplify the expression.\n",
        "\n",
        "**Answer:**\n",
        "*Your answer here*\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3TQQ9X30IJ27"
      },
      "source": [
        "### B.II - Show that the transition kernel satisfies the detailed balance condition\n",
        "\n",
        "Show that the transition kernel of the Metropolis algorithm satisfies the detailed balance condition with respect to $\\pi$, and thus $\\pi$ is its stationary distribution.\n",
        "Mathematically, you need to show that:\n",
        "\n",
        "$$\n",
        "p(x_{n+1}|x_n) \\pi(x_n) = p(x_n|x_{n+1}) \\pi(x_{n+1}).\n",
        "$$\n",
        "\n",
        "Hints:\n",
        "\n",
        "+ First prove that $a(x_n, x_{n+1})q(x_{n+1}|x_n)\\pi(x_n) = a(x_{n+1}, x_n)q(x_n|x_{n+1})\\pi(x_{n+1})$.\n",
        "+ Then, reuse the result above the symmetry of the Dirac delta function.\n",
        "\n",
        "**Answer:**\n",
        "*Your answer here*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f86tAv81IJ27"
      },
      "source": [
        "# Problem 2 - Mathematics of Variational Inference\n",
        "\n",
        "## Part A - Parameterization of a covariance matrix\n",
        "\n",
        "The purpose is to show that the commonly used rank-$k$ parameterization of the covariance matrix is indeed positive definite.\n",
        "\n",
        "Let $k$ be a positive integer, and $\\lambda_1, \\dots, \\lambda_k$ be real numbers.\n",
        "Let $d$ be another positive integer (the dimension of the covariance matrix) with $d \\geq k$.\n",
        "Let $u_1, \\dots, u_k$ be $d$-dimensional vectors, not necessarily orthogonal, but linearly independent.\n",
        "\n",
        "Consider the following matrix:\n",
        "\n",
        "$$\n",
        "\\Sigma = \\sum_{i=1}^k e^{\\lambda_i} u_i u_i^\\top.\n",
        "$$\n",
        "\n",
        "### A.I - Show that $\\Sigma$ is positive semi-definite.\n",
        "\n",
        "Hint: You need to show that for any non-zero vector $x \\in \\mathbb{R}^d$, the quadratic form $x^\\top \\Sigma x \\geq 0$.\n",
        "\n",
        "**Answer:**\n",
        "*Your answer here*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "egfC4V8cIJ28"
      },
      "source": [
        "### A.II - Numerical exploration of a rank-$k$ covariance matrix\n",
        "\n",
        "Set $d=100$ and $k=10$.\n",
        "Randomly generate $u_1, \\dots, u_k$ from the standard normal distribution.\n",
        "Randomly generate $\\lambda_1, \\dots, \\lambda_k$ from the standard normal distribution.\n",
        "Write Jax code (without a loop) to form the matrix $\\Sigma$ as defined above.\n",
        "Generate a random $\\Sigma$ and plot the eigenvalues.\n",
        "Are they all non-negative?\n",
        "What is the determinant of $\\Sigma$?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "StdvHAcWIJ28"
      },
      "outputs": [],
      "source": [
        "# as many code blocks and markdown blocks as you want\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2hDMPkyIJ29"
      },
      "source": [
        "### A.III - Low-rank approximation that is actually positive definite\n",
        "\n",
        "In the previous part, we saw that the rank-$k$ approximation is not positive definite.\n",
        "To fix it, we typically use this parameterization instead:\n",
        "\n",
        "$$\n",
        "\\Sigma = \\sum_{i=1}^k \\lambda_i u_i u_i^\\top + \\text{diag}(e^{\\theta_1}, \\dots, e^{\\theta_d}),\n",
        "$$\n",
        "\n",
        "where $\\theta_1, \\dots, \\theta_d$ are real numbers.\n",
        "\n",
        "Modify your Jax code and generate a random $\\Sigma$ using this parameterization.\n",
        "Plot the eigenvalues.\n",
        "Are they all non-negative?\n",
        "What is the determinant of $\\Sigma$?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lV3ZDzw6IJ29"
      },
      "outputs": [],
      "source": [
        "# as many code blocks and markdown blocks as you want"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "259DLOsFIJ2-"
      },
      "source": [
        "## Part B - Multi-point convexity\n",
        "\n",
        "Let $f:\\mathbb{R}^d \\to \\mathbb{R}$ be a convex function.\n",
        "Let $x_1, \\dots, x_n \\in \\mathbb{R}^d$ be $n$ points.\n",
        "Let $w_1, \\dots, w_n \\in \\mathbb{R}$ be $n$ weights.\n",
        "\n",
        "Show that:\n",
        "\n",
        "$$\n",
        "f\\left(\\sum_{i=1}^n w_i x_i\\right) \\leq \\sum_{i=1}^n w_i f(x_i).\n",
        "$$\n",
        "\n",
        "Hint: Use the definition of convexity and induction.\n",
        "\n",
        "**Answer:**\n",
        "*Your answer here*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8N0PD0-IJ2-"
      },
      "source": [
        "## Part C - Jensen's inequality\n",
        "\n",
        "Let $f:\\mathbb{R}^d \\to \\mathbb{R}$ be a convex function that is continuous.\n",
        "Let $X$ be a random variable with values in $\\mathbb{R}^d$.\n",
        "\n",
        "Show that:\n",
        "\n",
        "$$\n",
        "f(\\mathbb{E}[X]) \\leq \\mathbb{E}[f(X)].\n",
        "$$\n",
        "\n",
        "Hint: Use Part B and the law of large numbers.\n",
        "\n",
        "**Answer:**\n",
        "*Your answer here*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bmlH4zZAIJ2-"
      },
      "source": [
        "## Part D - Non-negativity of the KL divergence\n",
        "\n",
        "Let $p$ and $q$ be two probability distributions on $\\mathbb{R}^d$.\n",
        "Show that the KL divergence $D_{KL}(p\\|q)$ is always non-negative.\n",
        "\n",
        "Hint: Use the fact that $-\\log$ is a convex function and Jensen's inequality.\n",
        "\n",
        "**Answer:**\n",
        "*Your answer here*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iMc2Re-jIJ2-"
      },
      "source": [
        "# Problem 3 - Calibrating a pharmacokinetic model\n",
        "\n",
        "A pharmacokinetic (PK) compartment model is a set of ordinary differential equations that describe drug transport in the body.\n",
        "Typically, the body is divided into separate \"compartments\" (e.g., blood, peripheral tissues) and the transfer of the drug between these compartments is assumed to follow first-order kinetics.\n",
        "Consider the following two-compartment model for an *intravenous (IV) bolus* administered drug:\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\frac{dc_1}{dt} &= - k_{12} c_1 + k_{21} c_2\\\\\n",
        "\\frac{dc_2}{dt} &= k_{12} c_1 - (k_{21} + k_d) c_2 \\\\\n",
        "c_1(0) &= \\frac{m_\\text{dose}}{V} \\\\\n",
        "c_2(0) &= 0\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "where $c_1$ is the concentration of the drug in the *central compartment* (e.g., blood, kidney, liver),\n",
        "$c_2$ is the concentration in the *peripheral compartment* (e.g., muscle, fat),\n",
        "$k_i$ are the *rate constants*,\n",
        "$m_\\text{dose}$ is the mass of the drug administered,\n",
        "and $V$ is the *volume of distribution*.\n",
        "\n",
        "(In this context \"IV\" means the drug is injected directly into the bloodstream, and \"bolus\" means the drug is given all at once (instead of slowly administering it over minutes/hours).)\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/PredictiveScienceLab/advanced-scientific-machine-learning/refs/heads/main/book/images/compartment_model.png\" alt=\"pk_model\" width=\"400\"/>\n",
        "\n",
        "Here is an analytic solver for the PK model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MMhrRZrnIJ2_"
      },
      "outputs": [],
      "source": [
        "from jax import tree, jit, vmap\n",
        "import jax.numpy as jnp\n",
        "from functools import partial\n",
        "\n",
        "@partial(vmap, in_axes=(None, 0, None))\n",
        "def solve_pk_iv_bolus(params, time, dosage_mass):\n",
        "    k12, k21, kd = params['k12'], params['k21'], params['kd']\n",
        "    lam1 = (-(k12 + k21 + kd) + jnp.sqrt((k12 + k21 + kd)**2 - 4*k12*kd))/2\n",
        "    lam2 = (-(k12 + k21 + kd) - jnp.sqrt((k12 + k21 + kd)**2 - 4*k12*kd))/2\n",
        "    A = (dosage_mass / params['V'])/(1 - (lam1 + k12)/(lam2 + k12))\n",
        "    B = -(lam1 + k12)/(lam2 + k12)*A\n",
        "    c1 = A*jnp.exp(lam1*time) + B*jnp.exp(lam2*time)\n",
        "    return c1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbq1cmjgIJ2_"
      },
      "source": [
        "It accepts a dictionary of parameters, the vector of times, and an initial condition:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Mbc33zDIJ2_"
      },
      "outputs": [],
      "source": [
        "_params_test = {'k12': 0.1, 'k21': 0.2, 'kd': 0.3, 'V': 10.0}\n",
        "_times_test = jnp.linspace(0, 10, 20)\n",
        "_dosage_mass_test = 1.0\n",
        "\n",
        "_c1 = solve_pk_iv_bolus(_params_test, _times_test, _dosage_mass_test)\n",
        "_c1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iKj3QQAcIJ2_"
      },
      "source": [
        "Suppose a subject has received a dose of acetaminophen via IV bolus administration, and we have measured the drug concentration in the blood at discrete times.\n",
        "Let's import these data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8-VMBnpiIJ2_"
      },
      "outputs": [],
      "source": [
        "!curl -O 'https://raw.githubusercontent.com/PredictiveScienceLab/advanced-scientific-machine-learning/refs/heads/main/book/data/pk/iv_bolus_data_single_patient.json'\n",
        "\n",
        "import json\n",
        "import numpy as np\n",
        "\n",
        "with open('iv_bolus_data_single_patient.json', 'r') as f:\n",
        "    data = json.load(f)\n",
        "times = np.array(data['times'])\n",
        "concentrations = np.array(data['concentrations'])\n",
        "dosage_mass = data['dosage_mass']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVJ3LnL5IJ3A"
      },
      "source": [
        "Here are the observation times (in hours), $\\mathbf{t}=(t_1, \\dots, t_N)\\in\\mathbb{R}_+^N$:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P9B4lDVrIJ3A"
      },
      "outputs": [],
      "source": [
        "times"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TVft64I6IJ3A"
      },
      "source": [
        "Here are the observed concentrations at each time point (in gram/liter), $\\mathbf{y}=(y_1, \\dots, y_N) \\in \\mathbb{R}_+^N$:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DWg1KgaVIJ3A"
      },
      "outputs": [],
      "source": [
        "concentrations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LYU0BU6YIJ3A"
      },
      "source": [
        "And here is the dose (in grams), $m_\\text{dose}$:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BmqZXPncIJ3A"
      },
      "outputs": [],
      "source": [
        "dosage_mass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oBcJGHj7IJ3B"
      },
      "source": [
        "Let's plot the PK data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3uNMaLr5IJ3B"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(1, 1, figsize=(6, 4))\n",
        "ax.scatter(times, concentrations, label='Observed concentration', color='tab:blue')\n",
        "ax.set_xlabel('Time')\n",
        "ax.set_ylabel('Concentration')\n",
        "ax.legend()\n",
        "sns.despine()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DD15NBz3IJ3B"
      },
      "source": [
        "## Part A - Implement the (unnormalized) log posterior density\n",
        "\n",
        "Let $\\theta=(k_{12}, k_{21}, k_d, V) \\in \\mathbb{R}_+^4$ be the PK parameters, to which we'll assign weakly-informative priors\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "k_{12} &\\sim \\text{Exp}(0.1)\\\\\n",
        "k_{21} &\\sim \\text{Exp}(0.1)\\\\\n",
        "k_d &\\sim \\text{Exp}(0.1)\\\\\n",
        "V &\\sim \\text{LogNormal}(4, 0.5).\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "Also, let $c_1(t; \\theta, m_\\text{dose}) \\in \\mathbb{R}_+^3$ be the concentration in the central (bloodstream) compartment at time $t$ for initial condition $x_0=(m_\\text{dose}, 0) \\in \\mathbb{R}_+^2$.\n",
        "\n",
        "Assume the observations are independent, identically distributed Gaussian random variables, i.e.,\n",
        "\n",
        "$$\n",
        "y_i|t_i, \\theta, \\sigma \\sim \\text{Normal}\\Big(\\underbrace{c_1(t_i; \\theta, x_0)}_\\text{ODE solver output}, \\sigma^2 I\\Big).\n",
        "$$\n",
        "\n",
        "Suppose you know, from previous studies, that the measurement uncertainty is $\\sigma=0.001$.\n",
        "The (unnormalized) log posterior density function is\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "    \\underbrace{\\log p(\\theta|\\mathbf{t}, \\mathbf{y}, \\sigma)}_\\text{posterior}\n",
        "    &= \\underbrace{\\log p(\\theta)}_\\text{prior} + \\underbrace{\\log \\prod_{i=1}^N p(y_i | t_i, \\theta, \\sigma)}_\\text{likelihood} + \\underbrace{[\\text{constant terms w.r.t. } \\theta]}_\\text{normalizing contant (we can ignore)} \\\\\n",
        "    &\\propto \\log p(k_{12}) + \\log p(k_{21}) + \\log p(k_d) +\\log p(V) + \\sum_{i=1}^N \\log p(y_i | t_i, \\theta, \\sigma).\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "where we are defining $\\propto$ to mean \"equal up to a normalizing constant\".\n",
        "Now, it is easier to work with a set of \"unconstrained model parameters\" $\\xi$ that span all of $\\mathbb{R}^d$.\n",
        "To this end, define\n",
        "\n",
        "$$\n",
        "\\xi = \\log \\theta = (\\log k_{12}, \\log k_{21}, \\log k_d, \\log V) \\in \\mathbb{R}^4.\n",
        "$$\n",
        "\n",
        "**Your task is to implement the function that computes the (unnormalized) log posterior over $\\xi$**\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "    \\log p(\\xi | \\mathbf{t}, \\mathbf{y}, \\sigma)\n",
        "    &= \\log p(\\theta | \\mathbf{t}, \\mathbf{y}, \\sigma) + \\log \\det \\left| \\frac{\\partial \\theta}{\\partial \\xi} \\right|\\\\\n",
        "    &= \\log p(\\theta | \\mathbf{t}, \\mathbf{y}, \\sigma) + \\log \\prod_{i=1}^d \\frac{\\partial \\theta_i}{\\partial \\xi_i} \\\\\n",
        "    &= \\log p(\\xi) + \\log p(\\mathbf{y} | \\mathbf{t}, \\xi, \\sigma) + \\sum_{i=1}^d \\xi_i + \\underbrace{\\text{constant terms}}_\\text{ignore these}.\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "We've started it for you&mdash;just fill in the missing pieces of the code below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sivWH60dIJ3B"
      },
      "outputs": [],
      "source": [
        "sigma = 0.001\n",
        "\n",
        "def constrain(params):\n",
        "    \"\"\"Constrain the parameters to be positive.\"\"\"\n",
        "    return {\n",
        "        'k12': jnp.exp(params['k12']),\n",
        "        'k21': jnp.exp(params['k21']),\n",
        "        'kd': jnp.exp(params['kd']),\n",
        "        'V': jnp.exp(params['V']),\n",
        "    }\n",
        "\n",
        "def unconstrain(params):\n",
        "    \"\"\"Unconstrain the parameters to be real numbers.\"\"\"\n",
        "    return {\n",
        "        'k12': jnp.log(params['k12']),\n",
        "        'k21': jnp.log(params['k21']),\n",
        "        'kd': jnp.log(params['kd']),\n",
        "        'V': jnp.log(params['V']),\n",
        "    }\n",
        "\n",
        "@jit\n",
        "def log_posterior(xi, times=times, concentrations=concentrations, dosage_mass=dosage_mass, sigma=sigma):\n",
        "    \"\"\"Log likelihood function for a single individual's PK data.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    xi: dict\n",
        "        Dictionary containing the unconstrained parameters of the model:\n",
        "        - k12: transfer rate from compartment 1 to compartment 2\n",
        "        - k21: transfer rate from compartment 2 to compartment 1\n",
        "        - kd: elimination rate from compartment 2\n",
        "        - V: volume of distribution\n",
        "    times: array\n",
        "        Time points at which to evaluate the solution.\n",
        "    concentrations: array\n",
        "        Observed concentrations at the specified time points.\n",
        "    dosage_mass: float\n",
        "        Mass of the dosage administered.\n",
        "    sigma: float\n",
        "        Standard deviation of the measurement noise.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    float\n",
        "        Log likelihood of the observed data given the model parameters.\n",
        "    \"\"\"\n",
        "    # Log PDF functions for standard distributions\n",
        "    normal_log_pdf = lambda x, mu, sigma: (-0.5*jnp.log(2*jnp.pi)\n",
        "                                          - jnp.log(sigma)\n",
        "                                          - 0.5*((x - mu)/sigma)**2)\n",
        "\n",
        "    lognormal_log_pdf = lambda x, mu, sigma: (-0.5*jnp.log(2*jnp.pi)\n",
        "                                              - jnp.log(sigma) - jnp.log(x)\n",
        "                                              - 0.5*((jnp.log(x) - mu)/sigma)**2)\n",
        "\n",
        "    exponential_log_pdf = lambda x, rate: (jnp.log(rate) - rate*x)\n",
        "\n",
        "    # Transform to constrained space\n",
        "    params = constrain(xi)\n",
        "\n",
        "    # Prior\n",
        "    k12_log_prior = exponential_log_pdf(params['k12'], 0.1)\n",
        "    k21_log_prior = exponential_log_pdf(params['k21'], 0.1)   # Your code here\n",
        "    kd_log_prior = exponential_log_pdf(params['kd'], 0.1)  # Your code here\n",
        "    V_log_prior = lognormal_log_pdf(params['V'], mu=4.0, sigma=0.5)  # Your code here\n",
        "    log_prior = k12_log_prior + k21_log_prior + kd_log_prior + V_log_prior\n",
        "\n",
        "    # Likelihood\n",
        "    c1 = solve_pk_iv_bolus(params, times, dosage_mass)\n",
        "    log_likelihood = log_likelihood = jnp.sum(normal_log_pdf(concentrations, c1, sigma))  # Your code here\n",
        "\n",
        "    # Determinant of the Jacobian of the transformation\n",
        "\n",
        "\n",
        "    # log_det_jac =  xi['k12'] + xi['k21'] + xi['kd'] + xi['V']  # Your code here\n",
        "    log_det_jac = jnp.sum(xi['k12'] + xi['k21'] + xi['kd'] + xi['V'])\n",
        "\n",
        "    return log_prior + log_likelihood + log_det_jac"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mGracVVoIJ3C"
      },
      "source": [
        "Your implementation of $p(\\xi|\\mathbf{t}, \\mathbf{y}, \\sigma)$ above could also be done with a probabilistic programming framework.\n",
        "Here is how to do it with [Numpyro](https://num.pyro.ai/en/latest/index.html#):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "plDipddFIJ3C"
      },
      "outputs": [],
      "source": [
        "import jax.random as jr\n",
        "import numpyro\n",
        "import numpyro.distributions as dist\n",
        "from numpyro.infer import util\n",
        "\n",
        "def numpyro_model(times, concentrations, dosage_mass, sigma):\n",
        "    \"\"\"Model function for the JAX-based probabilistic model.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    times: array\n",
        "        Time points at which to evaluate the solution.\n",
        "    concentrations: array\n",
        "        Observed concentrations at the specified time points.\n",
        "    dosage_mass: float\n",
        "        Mass of the dosage administered.\n",
        "    sigma: float\n",
        "        Standard deviation of the measurement noise.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    float\n",
        "        Log likelihood of the observed data given the model parameters.\n",
        "    \"\"\"\n",
        "    # Prior\n",
        "    k12 = numpyro.sample('k12', dist.Exponential(0.1))\n",
        "    k21 = numpyro.sample('k21', dist.Exponential(0.1))\n",
        "    kd = numpyro.sample('kd', dist.Exponential(0.1))\n",
        "    V = numpyro.sample('V', dist.LogNormal(4, 0.5))\n",
        "\n",
        "    # Likelihood\n",
        "    params = {'k12': k12, 'k21': k21, 'kd': kd, 'V': V}\n",
        "    c1 = solve_pk_iv_bolus(params, times, dosage_mass)\n",
        "    with numpyro.plate('data', len(times)):\n",
        "        numpyro.sample('obs', dist.Normal(c1, sigma), obs=concentrations)\n",
        "\n",
        "model_default_args = (times, concentrations, dosage_mass, sigma)\n",
        "\n",
        "(\n",
        "    init_params,\n",
        "    potential_fn_gen,\n",
        "    postprocess_fn_gen,\n",
        "    model_trace\n",
        ") = util.initialize_model(\n",
        "    jr.key(0),\n",
        "    numpyro_model,\n",
        "    model_args=model_default_args,  # Dummy arguments\n",
        "    dynamic_args=True,\n",
        ")\n",
        "\n",
        "# Get the probability density.\n",
        "# This is p(ξ|y)\n",
        "log_posterior_numpyro = lambda x: -potential_fn_gen(*model_default_args)(x)\n",
        "\n",
        "# Get the transformation function.\n",
        "# This is ξ ↦ θ\n",
        "constrain_numpyro = jit(lambda x: util.constrain_fn(numpyro_model, model_default_args, {}, x))\n",
        "\n",
        "# And get the inverse transformation function.\n",
        "# This is θ ↦ ξ\n",
        "unconstrain_numpyro = jit(lambda x: util.unconstrain_fn(numpyro_model, model_default_args, {}, x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JRhGL_vwIJ3D"
      },
      "source": [
        "Run the cell below to test your implementation of $p(\\xi|\\mathbf{t}, \\mathbf{y}, \\sigma)$ against the Numpyro implementation. The printed results should be identical."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CF09OIDZIJ3D"
      },
      "outputs": [],
      "source": [
        "xi = {\n",
        "    'k12': 0.1,\n",
        "    'k21': 0.2,\n",
        "    'kd': 0.3,\n",
        "    'V': 10.0,\n",
        "}\n",
        "\n",
        "print('Log posterior evaluated from scratch: ', log_posterior(xi))\n",
        "print('Log posterior evaluated from numpyro: ', log_posterior_numpyro(xi))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8uIwbrkJIJ3D"
      },
      "source": [
        "## Part B - Plot the prior predictive distribution\n",
        "\n",
        "First, let's create a function that samples the random variable $\\xi$ (i.e., the prior over the unnormalized variables).\n",
        "\n",
        "Again, complete the missing pieces of the following code:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WE-N-pPGIJ3E"
      },
      "outputs": [],
      "source": [
        "@partial(jit, static_argnums=1)\n",
        "def sample_prior_xi(key, num_samples):\n",
        "    keys = jr.split(key, 4)\n",
        "    k12 = (1/0.1)*jr.exponential(keys[0], shape=(num_samples,))\n",
        "    k21 = (1/0.1)*jr.exponential(keys[1], shape=(num_samples,))  # Your code here\n",
        "    kd = (1/0.1)*jr.exponential(keys[2], shape=(num_samples,))  # Your code here\n",
        "    V = jnp.exp(4)*jr.lognormal(keys[3], shape=(num_samples,), sigma=0.5)\n",
        "    xi = {\n",
        "        'k12': jnp.log(k12),\n",
        "        'k21':jnp.log(k21), # Your code here\n",
        "        'kd':  jnp.log(kd),  # Your code here\n",
        "        'V': jnp.log(V),\n",
        "    }\n",
        "    return xi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dnp0s7QjIJ3E"
      },
      "source": [
        "And again, we could implement the same thing using Numpyro.\n",
        "Here is how you could use Numpyro's `Predictive` class to create a function that samples all latent variables defined in `numpyro_model`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rKjqxk5dIJ3E"
      },
      "outputs": [],
      "source": [
        "@partial(jit, static_argnums=1)\n",
        "def sample_prior_xi_numpyro(key, num_samples):\n",
        "    s = numpyro.infer.Predictive(numpyro_model, num_samples=num_samples)(key, *model_default_args)\n",
        "    xi = vmap(unconstrain_numpyro)(s)\n",
        "    xi = {k: v for k, v in xi.items() if k in init_params.z.keys()}\n",
        "    return xi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OJLjx9OKIJ3F"
      },
      "source": [
        "Run the following cell to test your prior sampler against Numpyro's prior sampler.\n",
        "The two should be essentially equivalent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "puP4LjbYIJ3G"
      },
      "outputs": [],
      "source": [
        "xi_prior_samples = sample_prior_xi(jr.key(0), 100_000)\n",
        "xi_prior_samples_numpyro = sample_prior_xi_numpyro(jr.key(0), 100_000)\n",
        "param_prior_samples = vmap(constrain)(xi_prior_samples)\n",
        "param_prior_samples_numpyro = vmap(constrain_numpyro)(xi_prior_samples_numpyro)\n",
        "\n",
        "fig, axes = plt.subplots(1, 4, figsize=(12, 2))\n",
        "\n",
        "ax = axes[0]\n",
        "sns.kdeplot(xi_prior_samples['k12'], label='from scratch', color='tab:blue', ax=ax)\n",
        "sns.kdeplot(xi_prior_samples_numpyro['k12'], label='from numpyro', color='tab:orange', linestyle='--', ax=ax)\n",
        "ax.set_xlabel('Parameter value')\n",
        "ax.set_title(r'Prior for $\\xi_{k_{12}}$')\n",
        "ax.legend()\n",
        "\n",
        "ax = axes[1]\n",
        "sns.kdeplot(xi_prior_samples['k21'], label='from scratch', color='tab:blue', ax=ax)\n",
        "sns.kdeplot(xi_prior_samples_numpyro['k21'], label='from numpyro', color='tab:orange', linestyle='--', ax=ax)\n",
        "ax.set_xlabel('Parameter value')\n",
        "ax.set_title(r'Prior for $\\xi_{k_{21}}$')\n",
        "ax.legend()\n",
        "\n",
        "ax = axes[2]\n",
        "sns.kdeplot(xi_prior_samples['kd'], label='from scratch', color='tab:blue', ax=ax)\n",
        "sns.kdeplot(xi_prior_samples_numpyro['kd'], label='from numpyro', color='tab:orange', linestyle='--', ax=ax)\n",
        "ax.set_xlabel('Parameter value')\n",
        "ax.set_title(r'Prior for $\\xi_{k_{d}}$')\n",
        "ax.legend()\n",
        "\n",
        "ax = axes[3]\n",
        "sns.kdeplot(xi_prior_samples['V'], label='from scratch', color='tab:blue', ax=ax)\n",
        "sns.kdeplot(xi_prior_samples_numpyro['V'], label='from numpyro', color='tab:orange', linestyle='--', ax=ax)\n",
        "ax.set_xlabel('Parameter value')\n",
        "ax.set_title(r'Prior for $\\xi_{V}$')\n",
        "ax.legend()\n",
        "\n",
        "sns.despine()\n",
        "\n",
        "fig, axes = plt.subplots(1, 4, figsize=(12, 2))\n",
        "\n",
        "ax = axes[0]\n",
        "sns.kdeplot(param_prior_samples['k12'], label='from scratch', color='tab:blue', ax=ax)\n",
        "sns.kdeplot(param_prior_samples_numpyro['k12'], label='from numpyro', color='tab:orange', linestyle='--', ax=ax)\n",
        "ax.set_xlabel('Parameter value')\n",
        "ax.set_title(r'Prior for $k_{12}$')\n",
        "ax.legend()\n",
        "\n",
        "ax = axes[1]\n",
        "sns.kdeplot(param_prior_samples['k21'], label='from scratch', color='tab:blue', ax=ax)\n",
        "sns.kdeplot(param_prior_samples_numpyro['k21'], label='from numpyro', color='tab:orange', linestyle='--', ax=ax)\n",
        "ax.set_xlabel('Parameter value')\n",
        "ax.set_title(r'Prior for $k_{21}$')\n",
        "ax.legend()\n",
        "\n",
        "ax = axes[2]\n",
        "sns.kdeplot(param_prior_samples['kd'], label='from scratch', color='tab:blue', ax=ax)\n",
        "sns.kdeplot(param_prior_samples_numpyro['kd'], label='from numpyro', color='tab:orange', linestyle='--', ax=ax)\n",
        "ax.set_xlabel('Parameter value')\n",
        "ax.set_title(r'Prior for $k_{d}$')\n",
        "ax.legend()\n",
        "\n",
        "ax = axes[3]\n",
        "sns.kdeplot(param_prior_samples['V'], label='from scratch', color='tab:blue', ax=ax)\n",
        "sns.kdeplot(param_prior_samples_numpyro['V'], label='from numpyro', color='tab:orange', linestyle='--', ax=ax)\n",
        "ax.set_xlabel('Parameter value')\n",
        "ax.set_title(r'Prior for $V$')\n",
        "ax.legend()\n",
        "\n",
        "sns.despine()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T7JZNMD_IJ3G"
      },
      "source": [
        "(From this point forward, feel free to use either your implementation of the log density and prior sampler or the Numpyro implementation.)\n",
        "\n",
        "Now, collect 1000 *prior* samples of $\\theta=e^\\xi$.\n",
        "Plot the the following over the time interval $[0, 12]$ hours:\n",
        "- The 95% credible interval for $c_1(t; \\theta, m_\\text{dose})$\n",
        "- The 95% predictive interval for $c_1(t; \\theta, m_\\text{dose}) + \\epsilon; ~ \\epsilon \\sim \\mathcal{N}(0, \\sigma)$\n",
        "- A few samples of $c_1(t; \\theta, m_\\text{dose})$\n",
        "\n",
        "Hint: You may simply run the following code cell to create the plots."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uIvNcZ_4IJ3G"
      },
      "outputs": [],
      "source": [
        "key, subkey = jr.split(jr.key(1))\n",
        "\n",
        "t_plt = jnp.linspace(0, 12, 100)\n",
        "xi_prior_samples = sample_prior_xi(jr.key(0), 1_000)\n",
        "theta_prior_samples = vmap(constrain)(xi_prior_samples)\n",
        "c1_prior_samples = vmap(solve_pk_iv_bolus, (0, None, None))(theta_prior_samples, t_plt, 1.0)\n",
        "y_prior_predictive_samples = c1_prior_samples + sigma*jr.normal(key, shape=c1_prior_samples.shape)\n",
        "\n",
        "q05_epistemic, q95_epistemic = jnp.quantile(c1_prior_samples, jnp.array([0.05, 0.95]), axis=0)\n",
        "q05_aleatoric, q95_aleatoric = jnp.quantile(y_prior_predictive_samples, jnp.array([0.05, 0.95]), axis=0)\n",
        "\n",
        "fig, ax = plt.subplots(1, 1, figsize=(6, 3))\n",
        "ax.plot(t_plt, c1_prior_samples[:10].T, color='tab:blue', lw=0.5, alpha=0.8)\n",
        "ax.fill_between(t_plt, q95_aleatoric, q95_epistemic, color='tab:orange', alpha=0.3, lw=0)\n",
        "ax.fill_between(t_plt, q95_epistemic, q05_epistemic, color='tab:blue', alpha=0.3, lw=0, label='Epistemic uncertainty')\n",
        "ax.fill_between(t_plt, q05_epistemic, q05_epistemic, color='tab:orange', alpha=0.3, lw=0, label='Aleatoric uncertainty')\n",
        "ax.set_xlabel('Time (hr)')\n",
        "ax.set_ylabel('Concentration (g/L)')\n",
        "ax.set_title('Prior samples', fontsize=14)\n",
        "ax.legend()\n",
        "sns.despine()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r507_YsWIJ3G"
      },
      "source": [
        "## Part C - Find the maximum a-posteriori estimate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4V-WbidIJ3H"
      },
      "source": [
        "Now, we want to find the maximum a-posteriori (MAP) estimate $\\xi^*$, i.e.,\n",
        "\n",
        "$$\n",
        "\\xi^* = \\arg \\max_\\xi p(\\xi|\\mathbf{t}, \\mathbf{y} , \\sigma).\n",
        "$$\n",
        "\n",
        "We now have an unconstrained optimization problem.\n",
        "\n",
        "**Do the following:**\n",
        "- Use ADAM to find $\\xi^*$. Show that the loss converges. You may want to run ADAM for a few different starting points to ensure you have found the global maximum.\n",
        "- Report the MAP estimate of the parameters *in constrained space*, i.e., $\\theta^*=e^{\\xi^*}$.\n",
        "- Plot the MAP estimate for the central compartment concentration $c_1(t; \\theta^*, m_\\text{dose})$ *and* the 95% predictive interval (aleatoric uncertainty). (Hint: See part B.)\n",
        "Overlay the observations on the plot.\n",
        "- Compute and report the *area under the curve* (AUC) of the $c_1$-$t$ curve from $t=0$ to $t=36$ hours, i.e.,\n",
        "\n",
        "$$\n",
        "\\int_0^{36} c_1(t; \\theta^*, m_\\text{dose}) dt.\n",
        "$$\n",
        "\n",
        "&emsp;&emsp;&ensp;(AUC is a measure of the *total drug exposure*, and it helps assess drug efficacy and safety.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HjFkbKa3IJ3H"
      },
      "source": [
        "**Your answer here:** <br><br><br><br><br>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import optax\n",
        "from jax import value_and_grad\n",
        "from tqdm import trange\n",
        "\n",
        "# # Initial guess\n",
        "xi_init = {\n",
        "    'k12': jnp.log(0.4),\n",
        "    'k21': jnp.log(0.5),\n",
        "    'kd': jnp.log(0.9),\n",
        "    'V': jnp.log(50.0),\n",
        "}\n",
        "\n",
        "# Convert dict to flat vector (for optimization)\n",
        "def flatten_dict(d):\n",
        "    return jnp.concatenate([d[k][None] for k in ['k12', 'k21', 'kd', 'V']])\n",
        "\n",
        "def unflatten_vec(vec):\n",
        "    return {'k12': vec[0], 'k21': vec[1], 'kd': vec[2], 'V': vec[3]}\n",
        "\n",
        "xi_vec = flatten_dict(xi_init)\n",
        "\n",
        "# Define optimizer\n",
        "optimizer = optax.adam(learning_rate=1e-2)\n",
        "opt_state = optimizer.init(xi_vec)\n",
        "\n",
        "@jit\n",
        "def step(xi_vec, opt_state):\n",
        "    xi_dict = unflatten_vec(xi_vec)\n",
        "    loss, grads = value_and_grad(lambda x: -log_posterior(unflatten_vec(x)))(xi_vec)\n",
        "    updates, opt_state = optimizer.update(grads, opt_state)\n",
        "    xi_vec = optax.apply_updates(xi_vec, updates)\n",
        "    return xi_vec, opt_state, loss\n",
        "\n",
        "# Run optimization\n",
        "n_steps = 2000\n",
        "losses = []\n",
        "\n",
        "print(\"--------------------\")\n",
        "for i in trange(n_steps):\n",
        "    xi_vec, opt_state, loss = step(xi_vec, opt_state)\n",
        "    losses.append(loss)\n",
        "\n",
        "xi_map = unflatten_vec(xi_vec)\n",
        "theta_map = constrain(xi_map)\n",
        "\n",
        "print(\"\\n --------------------\")\n",
        "print(\"MAP estimate (constrained θ):\", theta_map)\n",
        "# print(\"MAP estimate (unconstrained θ):\", unconstrain(theta_map))\n",
        "\n",
        "print(\"\\n--------------------\")\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "t_plt = jnp.linspace(0, 12, 100)\n",
        "c1_map = solve_pk_iv_bolus(theta_map, t_plt, dosage_mass)\n",
        "\n",
        "# Aleatoric noise\n",
        "key = jr.key(42)\n",
        "y_samples = c1_map[:, None] + sigma * jr.normal(key, (len(t_plt), 1000))\n",
        "y_q05, y_q95 = jnp.quantile(y_samples, jnp.array([0.05, 0.95]), axis=1)\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.plot(t_plt, c1_map, label='MAP Estimate', color='tab:blue')\n",
        "plt.fill_between(t_plt, y_q05, y_q95, color='tab:blue', alpha=0.3, label='95% Aleatoric Interval')\n",
        "plt.scatter(times, concentrations, color='black', s=10, label='Observations')\n",
        "plt.xlabel('Time [hr]')\n",
        "plt.ylabel('Concentration [mg/L]')\n",
        "plt.title('MAP Parameters')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n--------------------\")\n",
        "# Calculate the AUC\n",
        "auc_jax = 0.5 * jnp.sum((c1_map[1:] + c1_map[:-1]) * (t_plt[1:] - t_plt[:-1]))\n",
        "print(f\"AUC (0–36 hr) [JAX]: {auc_jax:.4f} mg·hr/L\")"
      ],
      "metadata": {
        "id": "azTxWXCq8u5s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BF_elSyxIJ3H"
      },
      "source": [
        "## Part D - Sample the posterior with MCMC\n",
        "\n",
        "Instead of only finding a point estimate of the parameters (as in Part C), we will characterize the full posterior distribution $p(\\xi|\\mathbf{t}, \\mathbf{y}, \\sigma)$.\n",
        "This will allow us to quantify our *epistemic* or *lack-of-data* uncertainty about the parameters.\n",
        "\n",
        "**Do the following:**\n",
        "- Sample the from posterior distribution $p(\\xi|\\mathbf{t}, \\mathbf{y}, \\sigma)$ using the No-U-Turn Sampler (NUTS). Use at least 3 chains and 1000 samples per chain. (Hint: See [this hands-on activity demonstrating NUTS in blackjax](https://predictivesciencelab.github.io/advanced-scientific-machine-learning/inverse/sampling/04_nuts_blackjax.html).)\n",
        "- Show the trace plots of the MCMC chains. Report MCMC diagnostics (R-hat, ESS). Argue whether the chains have converged. (Hint: Use `arviz.plot_trace` and `arviz.summary`.)\n",
        "- Plot all the $\\xi$ samples onto a scatterplot matrix. (Hint: Use `seaborn.pairplot`.)\n",
        "Comment on how identifiable the parameters are.\n",
        "- Compute the concentration $c_1$ for each posterior sample, over the time interval $[0, 12]$.\n",
        "Plot the 95% credible interval, 95% predictive interval, and a few samples from the posterior. (Hint: See Part B.)\n",
        "- Plot a histogram of the area under the curve (AUC) for the posterior samples (see part C for the definition of \"AUC\")."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CweWMIW5IJ3H"
      },
      "source": [
        "**Your answer here:** <br><br><br><br><br>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using example from https://predictivesciencelab.github.io/advanced-scientific-machine-learning/inverse/sampling/04_nuts_blackjax.html"
      ],
      "metadata": {
        "id": "_x1f8xllTNkm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_inference(\n",
        "    joint_log_prob: Callable,\n",
        "    sample_starting_point: Callable,\n",
        "    key: PRNGKeyArray,\n",
        "    num_warmup: int,\n",
        "    num_samples: int,\n",
        "    num_chains: int\n",
        "):\n",
        "    \"\"\"Do warmup with window adaptation (to tune step size and mass matrix) and then sample using NUTS.\n",
        "    Returns a tuple with the NUTS states and some info.\n",
        "    \"\"\"\n",
        "    # Initialize the window adaptation algorithm\n",
        "    warmup = blackjax.window_adaptation(blackjax.nuts, joint_log_prob)\n",
        "\n",
        "    # Initialize the MCMC chain\n",
        "    key, init_key, warmup_key, sample_key = jax.random.split(key, 4)\n",
        "    init_position = sample_starting_point(init_key, num_chains)\n",
        "\n",
        "\n",
        "\n",
        "    # Run the warmup phase for each chain\n",
        "    @jax.vmap\n",
        "    def call_warmup(seed, param):\n",
        "        \"\"\"Run the warmup phase for a single chain.\"\"\"\n",
        "        (initial_states, tuned_params), _ = warmup.run(seed, param, num_warmup)\n",
        "        return initial_states, tuned_params\n",
        "    warmup_keys = jax.random.split(warmup_key, num_chains)\n",
        "    initial_states, tuned_params = jax.jit(call_warmup)(warmup_keys, init_position)\n",
        "\n",
        "    # Run the sampling phase\n",
        "    states, infos = inference_loop_multiple_chains(sample_key, initial_states, tuned_params, joint_log_prob, num_samples, num_chains)\n",
        "\n",
        "    # `states` contains the samples, `infos` contains other information about the sampling process\n",
        "    return states, infos\n",
        "\n",
        "\n",
        "def sample_starting_point(\n",
        "    key: PRNGKeyArray,\n",
        "    num_chains: int,\n",
        "    mu: Array,\n",
        "    Sigma: Array\n",
        "):\n",
        "    \"\"\"Draw samples from a multivariate normal as starting points for the MCMC chains.\n",
        "    Returns an array of shape (num_chains, num_dims).\n",
        "    \"\"\"\n",
        "    keys = jrandom.split(key, num_chains)\n",
        "    return vmap(lambda k: jrandom.multivariate_normal(k, mu, Sigma))(keys)\n",
        "\n",
        "\n",
        "def inference_loop_multiple_chains(\n",
        "    key: PRNGKeyArray,\n",
        "    initial_states: PyTree,\n",
        "    tuned_params: dict,\n",
        "    log_prob_fn: Callable,\n",
        "    num_samples: int,\n",
        "    num_chains: int\n",
        "):\n",
        "    \"\"\"Do NUTS sampling for multiple chains in a vectorized fashion. Returns a tuple with the NUTS states and some info.\"\"\"\n",
        "    # Initialize the NUTS kernel\n",
        "    kernel = blackjax.nuts.build_kernel()\n",
        "\n",
        "    def step_fn(key, state, **params):\n",
        "        \"\"\"A single step of NUTS for one chain.\"\"\"\n",
        "        return kernel(key, state, log_prob_fn, **params)\n",
        "\n",
        "    def one_step(states, key):\n",
        "        \"\"\"A single step of NUTS for multiple chains.\"\"\"\n",
        "        keys = jax.random.split(key, num_chains)\n",
        "        states, infos = jax.vmap(step_fn)(keys, states, **tuned_params)\n",
        "        return states, (states, infos)\n",
        "\n",
        "    # Run the NUTS sampling for multiple chains\n",
        "    keys = jax.random.split(key, num_samples)\n",
        "    _, (states, infos) = jax.lax.scan(one_step, initial_states, keys)\n",
        "\n",
        "    return (states, infos)\n"
      ],
      "metadata": {
        "id": "6QmehrJioVHK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import blackjax\n",
        "import jax.random as jr\n",
        "\n",
        "# We'll use the same log_posterior from Part C\n",
        "key = jr.key(123)\n",
        "num_chains = 3\n",
        "num_warmup = 500\n",
        "num_samples = 1000\n",
        "\n",
        "# Starting points\n",
        "mu = flatten_dict(xi_map)\n",
        "Sigma = 0.1 * jnp.eye(len(mu))\n",
        "\n",
        "sample_start = lambda key, n: sample_starting_point(key, n, mu=mu, Sigma=Sigma)\n",
        "\n",
        "states, infos = run_inference(\n",
        "    joint_log_prob=lambda x: log_posterior(unflatten_vec(x)),\n",
        "    sample_starting_point=sample_start,\n",
        "    key=key,\n",
        "    num_warmup=num_warmup,\n",
        "    num_samples=num_samples,\n",
        "    num_chains=num_chains\n",
        ")"
      ],
      "metadata": {
        "id": "wWTAxIuwE3gB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def arviz_trace_from_states(states, info, burn_in=0):\n",
        "    position = states.position\n",
        "    if isinstance(position, jax.Array):  # if states.position is array of samples\n",
        "        position = dict(samples=position)\n",
        "    else:\n",
        "        try:\n",
        "            position = position._asdict()\n",
        "        except AttributeError:\n",
        "            pass\n",
        "\n",
        "    samples = {}\n",
        "    for param in position.keys():\n",
        "        ndims = len(position[param].shape)\n",
        "        if ndims >= 2:\n",
        "            samples[param] = jnp.swapaxes(position[param], 0, 1)[\n",
        "                :, burn_in:\n",
        "            ]  # swap n_samples and n_chains\n",
        "            divergence = jnp.swapaxes(info.is_divergent[burn_in:], 0, 1)\n",
        "\n",
        "        if ndims == 1:\n",
        "            divergence = info.is_divergent\n",
        "            samples[param] = position[param]\n",
        "\n",
        "    trace_posterior = az.convert_to_inference_data(samples)\n",
        "    trace_sample_stats = az.convert_to_inference_data(\n",
        "        {\"diverging\": divergence}, group=\"sample_stats\"\n",
        "    )\n",
        "    trace = az.concat(trace_posterior, trace_sample_stats)\n",
        "    return trace\n",
        "\n",
        "trace = arviz_trace_from_states(states, infos)\n",
        "summ_df = az.summary(trace)\n",
        "summ_df"
      ],
      "metadata": {
        "id": "MhK57nAVSoXE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "az.plot_trace(trace)\n",
        "plt.tight_layout();"
      ],
      "metadata": {
        "id": "DK5We1GMS65D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "samples = states.position.transpose(num_chains, num_samples, 2)\n",
        "compute_diagnostics_every = 25\n",
        "rhats = []\n",
        "for i in range(2, num_samples_per_chain, compute_diagnostics_every):\n",
        "    rhat = blackjax.diagnostics.potential_scale_reduction(samples[:, :i])\n",
        "    rhats.append(rhat)\n",
        "rhats = jnp.array(rhats)"
      ],
      "metadata": {
        "id": "epMDRP_OS-1e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZDYQmshAIJ3H"
      },
      "source": [
        "## Part E - Variational inference with mean-field Gaussian guide\n",
        "\n",
        "**Do the following:**\n",
        "- Construct a mean-field multivariate Gaussian guide (i.e., diagonal covariance matrix)\n",
        "\n",
        "$$\n",
        "q_\\phi(\\xi)=\\mathcal{N}(\\xi| \\mu_\\phi, \\sigma_\\phi I).\n",
        "$$\n",
        "\n",
        "- Maximize the Evidence Lower Bound (ELBO) with respect to the guide parameters $\\phi$ so that the guide approximates the posterior, i.e.\n",
        "\n",
        "$$\n",
        "q_\\phi\\approx p(\\xi|\\mathbf{t}, \\mathbf{y}, \\sigma).\n",
        "$$\n",
        "&emsp;&emsp;&ensp;(Hint: See [this hands-on activity implementing VI with a full-rank Gaussian guide](https://predictivesciencelab.github.io/advanced-scientific-machine-learning/inverse/vi/02_catalysis.html).\n",
        "If you use `FullRankGaussianGuide` from the hands-on activity, at a minimum you will need to modify `Sigma`, `forward`, and `get_num_guide_params` to match the form of a mean-field Gaussian guide.\n",
        ")\n",
        "\n",
        "- Argue whether the optimization converged.\n",
        "- Collect 5,000 (approximate) posterior samples of $\\xi$ from the trained guide.\n",
        "- Plot all the $\\xi$ samples onto a scatterplot matrix.\n",
        "Overlay the MCMC samples from part D.\n",
        "Use transparency so that both VI and MCMC samples are visible.\n",
        "How well does mean-field Gaussian VI approximate the posterior?\n",
        "- As in parts B and D, compute the concentration $c_1$ for each posterior sample, over the time interval $[0, 12]$.\n",
        "Plot the 95% credible interval, 95% predictive interval, and a few samples from the posterior.\n",
        "- Plot a histogram of the AUCs of the posterior samples.\n",
        "Overlay (with transparency) the AUC histogram from part D (MCMC).\n",
        "Do they match?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JrX66V17IJ3H"
      },
      "source": [
        "**Your answer here:** <br><br><br><br><br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8VF1GuepIJ3H"
      },
      "source": [
        "## Part F - Variational inference with full-rank Gaussian guide\n",
        "\n",
        "**Do the following:**\n",
        "- Construct a full-rank multivariate Gaussian guide\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "q_\\phi(\\xi) &=\\mathcal{N}(\\xi| \\mu_\\phi, \\Sigma_\\phi) \\\\\n",
        "\\Sigma_\\phi &= L_\\phi L_\\phi^T \\\\\n",
        "\\end{align*}\n",
        "$$\n",
        "&emsp;&emsp;&ensp;where $L_\\phi$ is a lower-triangular matrix parameterized by $\\phi$.\n",
        "\n",
        "- Repeat the steps from part E (including the plots) using the new guide.\n",
        "- How do the full-rank and mean-field VI approximations compare?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2tUA8YKKIJ3I"
      },
      "source": [
        "**Your answer here:** <br><br><br><br><br>"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}